{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All imports specified here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "from urllib import parse\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import ast\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All global variables are declared here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds_path = \"DS/\"\n",
    "wh_json_path = all_ds_path + \"watch-history.json\"\n",
    "tags_html_tbl_path = all_ds_path + \"topics.html\" \n",
    "vd_resp_path = \"VideoDetailsResponses/\"\n",
    "cd_resp_path = \"ChannelDetailResponses/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All functions definations are present here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_id_from_url(youtubeurl):\n",
    "    video_id = parse.parse_qs(parse.urlparse(youtubeurl).query)['v'][0]\n",
    "    return video_id\n",
    "\n",
    "def create_youtube_client(api_key):\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey = api_key)\n",
    "    return youtube\n",
    "\n",
    "def get_video_details(videoId):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails,status\",\n",
    "        id=videoId,\n",
    "        fields=\"items(id,snippet(title,description,defaultLanguage,defaultAudioLanguage,channelId,channelTitle,tags,categoryId,liveBroadcastContent,publishedAt),contentDetails,statistics,status)\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "def get_channel_details(channelId):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,topicDetails,statistics,brandingSettings,status\",\n",
    "        id=channelId,\n",
    "        fields=\"items(id,snippet(title,description,defaultLanguage,customUrl,publishedAt,country),brandingSettings/channel/keywords,statistics,status)\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "def get_all_categories_details(cats):\n",
    "    \n",
    "    if len(cats) == 0:\n",
    "        request = youtube.videoCategories().list(\n",
    "            part=\"snippet\",\n",
    "            regionCode = \"IN\",\n",
    "            fields=\"items(id,snippet)\"\n",
    "        )\n",
    "    else:\n",
    "        param_name = \"id\"\n",
    "        param_value = cats\n",
    "        request = youtube.videoCategories().list(\n",
    "            part=\"snippet\",\n",
    "            id = cats,\n",
    "            fields=\"items(id,snippet)\"\n",
    "        )\n",
    "    \n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "def get_all_language_details():\n",
    "    request = youtube.i18nLanguages().list(\n",
    "        part=\"snippet\",\n",
    "        fields=\"items(id,snippet)\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "def get_all_region_details():\n",
    "    request = youtube.i18nRegions().list(\n",
    "        part=\"snippet\",\n",
    "        fields=\"items(id,snippet)\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response['items']\n",
    "\n",
    "def flatten_json_to_series(json):\n",
    "    return json_normalize(json).squeeze()\n",
    "\n",
    "def convert_csv_json_df(df,start):\n",
    "    gdf = pd.DataFrame()\n",
    "    i = 0\n",
    "    for row in df.itertuples():\n",
    "#         print(i, start)\n",
    "        org_json =  ast.literal_eval(row.c0)\n",
    "        cdf = pd.DataFrame(flatten_json_to_series(org_json))\n",
    "        cdf.rename(columns={0:start},inplace=True)\n",
    "        gdf = pd.concat([gdf,cdf],axis=1,sort=False)\n",
    "        i+=1\n",
    "        start+=1\n",
    "    \n",
    "    fcdf = gdf.T\n",
    "    return fcdf\n",
    "\n",
    "def prepare_csvs_details_list(fld_path):\n",
    "    files = os.listdir(fld_path)\n",
    "    files = [os.path.join(fld_path, f) for f in files] # add path to each file\n",
    "    files.sort(key=lambda x: os.path.getmtime(x))\n",
    "    \n",
    "    flist = []\n",
    "    for file in files:\n",
    "        f_copy = file;\n",
    "\n",
    "    #     f_copy = \"VideoDetailsResponses/video_details_800_1600_json.csv\"\n",
    "#         f_copy = f_copy.replace(\"VideoDetailsResponses/video_details_\",\"\") #see how this can be handled\n",
    "#         fstr = f_copy.replace(\"_json.csv\",\"\")\n",
    "        \n",
    "        f_copy = f_copy[(re.search(\"\\d\", f_copy)).start() : ]\n",
    "        fstr = f_copy.replace(\"_json.csv\",\"\")\n",
    "        \n",
    "        tlst = fstr.split(\":\")\n",
    "        if len(tlst) == 1:\n",
    "            tlst = fstr.split(\"_\")\n",
    "\n",
    "        start = tlst[0]\n",
    "        end = tlst[1]\n",
    "    \n",
    "        lst = []\n",
    "        lst.append(file)\n",
    "        lst.append(start)\n",
    "        lst.append(end)\n",
    "        flist.append(lst)\n",
    "        \n",
    "    return flist\n",
    "\n",
    "def combine_csvs(final_df,wh_df,col_name,fld_path):\n",
    "    flist = prepare_csvs_details_list(fld_path)\n",
    "#     print(flist)\n",
    "    \n",
    "    for lst in flist:\n",
    "        print(\"processing \", lst[0], final_df.shape)\n",
    "        start = int(lst[1])\n",
    "        end = int(lst[2])\n",
    "        wh_df_part = wh_df[start:end].copy()\n",
    "        json_res_ds_path = lst[0]\n",
    "        \n",
    "        json_res_ds_df = pd.read_csv(json_res_ds_path)\n",
    "        temp_df = pd.DataFrame(json_res_ds_df[col_name]).rename(columns={col_name:\"c0\"})\n",
    "        pdf = convert_csv_json_df(temp_df,start)\n",
    "        semifinal_df = pd.concat([wh_df_part,pdf],axis=1)\n",
    "        final_df = final_df.append(semifinal_df,sort=False)\n",
    "    \n",
    "    if 'index' in final_df:\n",
    "        final_df.drop(columns=['index'],inplace=True)\n",
    "        \n",
    "    final_df.sort_index(inplace=True)\n",
    "    return final_df\n",
    "\n",
    "def assign_parent_tag(tags_df,tag_cat_indcs):\n",
    "    for idx in range(len(tag_cat_indcs)):\n",
    "        start = tag_cat_indcs[idx]\n",
    "        \n",
    "        if idx+1 >= len(tag_cat_indcs):\n",
    "            end = tags_df.last_valid_index()\n",
    "        else:\n",
    "            end = tag_cat_indcs[idx+1]\n",
    "#         print(start,end)\n",
    "        \n",
    "        p_tag = tags_df.loc[start][0]\n",
    "#         print(p_tag)\n",
    "\n",
    "        for i in range(start,end,1):\n",
    "            tags_df.loc[tags_df.index[i], 'tag_parent'] = p_tag\n",
    "#             print(i)\n",
    "    \n",
    "    tags_df.drop(tags_df[tags_df[1].isnull()].index,inplace=True)\n",
    "    tags_df.rename(columns={0:\"tag_id\",1:\"tag_name\"},inplace=True)\n",
    "    return tags_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create youtube client using API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"******************************\"\n",
    "youtube = create_youtube_client(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read youtube watch history json file exported from google takeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.read_json(wh_json_path)\n",
    "# json_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean removed video entries, entries related to visits of youtube music, subtitles are empty(channel details are not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos which has been removed, visited youtube music, subtitles are empty(channel details are not available)\n",
    "\n",
    "title_url_null = json_df['titleUrl'].isnull()\n",
    "subtitles_null = json_df['subtitles'].isnull()\n",
    "# title_removed = json_df['title'] != 'Watched a video that has been removed')\n",
    "\n",
    "json_bad_df = json_df[title_url_null | subtitles_null]\n",
    "\n",
    "json_clean_df = json_df.drop(json_bad_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create usable dataframe of youtube watch history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_df = json_clean_df[['time','titleUrl']].reset_index()\n",
    "wh_df['videoId'] = wh_df['titleUrl'].apply(lambda x:get_video_id_from_url(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting All Video Details from Youtube API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get video details of a part of whole video dataframe by hitting youtube video api. \n",
    "###  <font color='red'> Don't run below cell if already have dataset, this will download data by hitting youtube API and have cost associated </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start = 0\n",
    "df_end = 800\n",
    "\n",
    "vd_csv_path = vd_resp_path + \"video_details_\" + str(df_start) + \":\" + str(df_end) + \"_json.csv\" \n",
    "\n",
    "wh_df_part = wh_df[df_start:df_end].copy()\n",
    "wh_df_part['video_details'] = wh_df_part['videoId'].apply(lambda x:get_video_details(x))\n",
    "\n",
    "wh_df_part.to_csv(vd_csv_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine all individual collected video details csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vd_df = pd.DataFrame()\n",
    "final_vd_df = combine_csvs(final_vd_df,wh_df,\"video_details\",vd_resp_path)\n",
    "# final_vd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export all collected video details to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vd_df.to_csv(all_ds_path + \"mohit_youtube_wh_ds.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all Unique Channels Details from Youtube API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all unique channel ids from videos in final watch history dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chnl_lst_df = pd.DataFrame(final_vd_df['snippet.channelId'].unique(),columns=['channelId'])\n",
    "# chnl_lst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get channel details of a part of whole channel id dataframe by hitting youtube video api\n",
    "### <font color='red'> Don't run below cell if already have dataset, this will download data by hitting youtube API and have cost associated</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start = 4500\n",
    "df_end = 5025\n",
    "\n",
    "cd_csv_path = cd_resp_path + \"channel_details_\" + str(df_start) + \":\" + str(df_end) + \"_json.csv\" \n",
    "\n",
    "chnl_lst_df_part = chnl_lst_df[df_start:df_end].copy()\n",
    "chnl_lst_df_part['channel_details'] = chnl_lst_df_part['channelId'].apply(lambda x:get_channel_details(x))\n",
    "\n",
    "chnl_lst_df_part.to_csv(cd_csv_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chnl_lst_df_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine all individual collected channel details csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_final_df = pd.DataFrame()\n",
    "\n",
    "channel_final_df = combine_csvs(channel_final_df,chnl_lst_df,\"channel_details\",cd_resp_path)\n",
    "\n",
    "channel_final_df.to_csv(all_ds_path + \"mohit_channel_details.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting details about unique categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting unique category ids from videos dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_cat_lst = list(final_vd_df['snippet.categoryId'].unique())\n",
    "results = [int(i) for i in unq_cat_lst if str(i)!='nan']\n",
    "cats_string = \",\".join( map(str, sorted(set(results)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting information about category from category id using youtube api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_response = get_all_categories_details(cats_string)\n",
    "# cat_response\n",
    "cat_df = flatten_json_to_series(cat_response)\n",
    "cat_df.to_csv(all_ds_path + \"mohit_all_categories.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_response = get_all_categories_details(\"\")\n",
    "# cat_response\n",
    "cat_df = flatten_json_to_series(cat_response)\n",
    "cat_df.to_csv(all_ds_path + \"all_in_categories.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting details about topics of a channel (topic ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading static details of topic details present  at https://developers.google.com/youtube/v3/docs/channels#topicDetails.topicIds[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.read_html(tags_html_tbl_path)[0]\n",
    "tags_df.drop(0,inplace=True)\n",
    "# tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting indices of parent tags from tags dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_cat_indcs = list(tags_df[tags_df[1].isnull()].index)\n",
    "# tag_cat_indcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign parent categories using info from dataframe itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tags_df = assign_parent_tag(tags_df,tag_cat_indcs)\n",
    "final_tags_df.to_csv(all_ds_path + \"tags.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting details about all youtube languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_response = get_all_language_details()\n",
    "# lang_response\n",
    "lang_df = flatten_json_to_series(lang_response)\n",
    "lang_df.to_csv(all_ds_path + \"lang_details.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting details about all youtube regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_response = get_all_region_details()\n",
    "# region_response\n",
    "regions_df = flatten_json_to_series(region_response)\n",
    "regions_df.to_csv(all_ds_path + \"region_details.csv\",index=False)\n",
    "# regions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Analysis of Data\n",
    "### Start from here if already have data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vd_df = pd.read_csv(all_ds_path + \"mohit_youtube_wh_ds.csv\")\n",
    "channel_final_df = pd.read_csv(all_ds_path + \"mohit_channel_details.csv\")\n",
    "cat_df = pd.read_csv(all_ds_path + \"mohit_all_categories.csv\")\n",
    "final_tags_df = pd.read_csv(all_ds_path + \"tags.csv\")\n",
    "lang_df = pd.read_csv(all_ds_path + \"lang_details.csv\")\n",
    "regions_df = pd.read_csv(all_ds_path + \"region_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_vd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contentDetails.caption                       2\n",
       "contentDetails.contentRating.ytRating        1\n",
       "contentDetails.definition                    2\n",
       "contentDetails.dimension                     2\n",
       "contentDetails.duration                   2359\n",
       "contentDetails.licensedContent               2\n",
       "contentDetails.projection                    2\n",
       "id                                       14789\n",
       "snippet.categoryId                          17\n",
       "snippet.channelId                         5024\n",
       "snippet.channelTitle                      5019\n",
       "snippet.defaultAudioLanguage                34\n",
       "snippet.defaultLanguage                     19\n",
       "snippet.description                      13152\n",
       "snippet.liveBroadcastContent                 2\n",
       "snippet.publishedAt                      14444\n",
       "snippet.title                            14745\n",
       "statistics.commentCount                   5496\n",
       "statistics.dislikeCount                   5806\n",
       "statistics.favoriteCount                     1\n",
       "statistics.likeCount                     10931\n",
       "statistics.viewCount                     15576\n",
       "status.embeddable                            2\n",
       "status.license                               2\n",
       "status.privacyStatus                         2\n",
       "status.publicStatsViewable                   2\n",
       "status.uploadStatus                          2\n",
       "time                                     17420\n",
       "titleUrl                                 14823\n",
       "videoId                                  14823\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vd_df[final_vd_df.columns.difference(['snippet.tags','contentDetails.regionRestriction.allowed','contentDetails.regionRestriction.blocked'])].nunique()\n",
    "\n",
    "# final_vd_df['snippet.defaultLanguage'].unique()\n",
    "\n",
    "# final_vd_df[final_vd_df['contentDetails.dimension'] == '3d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
